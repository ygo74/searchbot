[
    {
        "id": "azure_ai",
        "user_id": "4b697efe-e374-45ea-b662-f100c072a627",
        "name": "azure_ai",
        "type": "pipe",
        "content": "\"\"\"\ntitle: Azure AI Foundry Pipeline\nauthor: owndev\nauthor_url: https://github.com/owndev\nproject_url: https://github.com/owndev/Open-WebUI-Functions\nfunding_url: https://github.com/owndev/Open-WebUI-Functions\nversion: 2.2.0\nlicense: Apache License 2.0\ndescription: A pipeline for interacting with Azure AI services, enabling seamless communication with various AI models via configurable headers and robust error handling. This includes support for Azure OpenAI models as well as other Azure AI models by dynamically managing headers and request configurations.\nfeatures:\n  - Supports dynamic model specification via headers.\n  - Filters valid parameters to ensure clean requests.\n  - Handles streaming and non-streaming responses.\n  - Provides flexible timeout and error handling mechanisms.\n  - Compatible with Azure OpenAI and other Azure AI models.\n  - Predefined models for easy access.\n  - Encrypted storage of sensitive API keys\n\"\"\"\n\nfrom typing import List, Union, Generator, Iterator, Optional, Dict, Any\nfrom fastapi.responses import StreamingResponse\nfrom pydantic import BaseModel, Field, GetCoreSchemaHandler\nfrom starlette.background import BackgroundTask\nfrom open_webui.env import AIOHTTP_CLIENT_TIMEOUT, SRC_LOG_LEVELS\nfrom cryptography.fernet import Fernet, InvalidToken\nimport aiohttp\nimport json\nimport os\nimport logging\nimport base64\nimport hashlib\nfrom pydantic_core import core_schema\n\n\n# Simplified encryption implementation with automatic handling\nclass EncryptedStr(str):\n    \"\"\"A string type that automatically handles encryption/decryption\"\"\"\n\n    @classmethod\n    def _get_encryption_key(cls) -> Optional[bytes]:\n        \"\"\"\n        Generate encryption key from WEBUI_SECRET_KEY if available\n        Returns None if no key is configured\n        \"\"\"\n        secret = os.getenv(\"WEBUI_SECRET_KEY\")\n        if not secret:\n            return None\n\n        hashed_key = hashlib.sha256(secret.encode()).digest()\n        return base64.urlsafe_b64encode(hashed_key)\n\n    @classmethod\n    def encrypt(cls, value: str) -> str:\n        \"\"\"\n        Encrypt a string value if a key is available\n        Returns the original value if no key is available\n        \"\"\"\n        if not value or value.startswith(\"encrypted:\"):\n            return value\n\n        key = cls._get_encryption_key()\n        if not key:  # No encryption if no key\n            return value\n\n        f = Fernet(key)\n        encrypted = f.encrypt(value.encode())\n        return f\"encrypted:{encrypted.decode()}\"\n\n    @classmethod\n    def decrypt(cls, value: str) -> str:\n        \"\"\"\n        Decrypt an encrypted string value if a key is available\n        Returns the original value if no key is available or decryption fails\n        \"\"\"\n        if not value or not value.startswith(\"encrypted:\"):\n            return value\n\n        key = cls._get_encryption_key()\n        if not key:  # No decryption if no key\n            return value[len(\"encrypted:\") :]  # Return without prefix\n\n        try:\n            encrypted_part = value[len(\"encrypted:\") :]\n            f = Fernet(key)\n            decrypted = f.decrypt(encrypted_part.encode())\n            return decrypted.decode()\n        except (InvalidToken, Exception):\n            return value\n\n    # Pydantic integration\n    @classmethod\n    def __get_pydantic_core_schema__(\n        cls, _source_type: Any, _handler: GetCoreSchemaHandler\n    ) -> core_schema.CoreSchema:\n        return core_schema.union_schema(\n            [\n                core_schema.is_instance_schema(cls),\n                core_schema.chain_schema(\n                    [\n                        core_schema.str_schema(),\n                        core_schema.no_info_plain_validator_function(\n                            lambda value: cls(cls.encrypt(value) if value else value)\n                        ),\n                    ]\n                ),\n            ],\n            serialization=core_schema.plain_serializer_function_ser_schema(\n                lambda instance: str(instance)\n            ),\n        )\n\n    def get_decrypted(self) -> str:\n        \"\"\"Get the decrypted value\"\"\"\n        return self.decrypt(self)\n\n\n# Helper functions\nasync def cleanup_response(\n    response: Optional[aiohttp.ClientResponse],\n    session: Optional[aiohttp.ClientSession],\n) -> None:\n    \"\"\"\n    Clean up the response and session objects.\n\n    Args:\n        response: The ClientResponse object to close\n        session: The ClientSession object to close\n    \"\"\"\n    if response:\n        response.close()\n    if session:\n        await session.close()\n\n\nclass Pipe:\n    # Environment variables for API key, endpoint, and optional model\n    class Valves(BaseModel):\n        # API key for Azure AI\n        AZURE_AI_API_KEY: EncryptedStr = Field(\n            default=os.getenv(\"AZURE_AI_API_KEY\", \"API_KEY\"),\n            description=\"API key for Azure AI\",\n        )\n\n        # Endpoint for Azure AI (e.g. \"https://<your-endpoint>/chat/completions?api-version=2024-05-01-preview\" or \"https://<your-endpoint>/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview\")\n        AZURE_AI_ENDPOINT: str = Field(\n            default=os.getenv(\n                \"AZURE_AI_ENDPOINT\",\n                \"https://<your-endpoint>/chat/completions?api-version=2024-05-01-preview\",\n            ),\n            description=\"Endpoint for Azure AI\",\n        )\n\n        # Optional model name, only necessary if not Azure OpenAI or if model name not in URL (e.g. \"https://<your-endpoint>/openai/deployments/<model-name>/chat/completions\")\n        # Multiple models can be specified as a semicolon-separated list (e.g. \"gpt-4o;gpt-4o-mini\")\n        # or a comma-separated list (e.g. \"gpt-4o,gpt-4o-mini\").\n        AZURE_AI_MODEL: str = Field(\n            default=os.getenv(\"AZURE_AI_MODEL\", \"\"),\n            description=\"Optional model names for Azure AI (e.g. gpt-4o, gpt-4o-mini)\",\n        )\n\n        # Switch for sending model name in request body\n        AZURE_AI_MODEL_IN_BODY: bool = Field(\n            default=os.getenv(\"AZURE_AI_MODEL_IN_BODY\", False),\n            description=\"If True, include the model name in the request body instead of as a header.\",\n        )\n\n        # Flag to indicate if predefined Azure AI models should be used\n        USE_PREDEFINED_AZURE_AI_MODELS: bool = Field(\n            default=os.getenv(\"USE_PREDEFINED_AZURE_AI_MODELS\", False),\n            description=\"Flag to indicate if predefined Azure AI models should be used.\",\n        )\n\n        # If True, use Authorization header with Bearer token instead of api-key header.\n        USE_AUTHORIZATION_HEADER: bool = Field(\n            default=bool(os.getenv(\"AZURE_AI_USE_AUTHORIZATION_HEADER\", False)),\n            description=\"Set to True to use Authorization header with Bearer token instead of api-key header.\",\n        )\n\n    def __init__(self):\n        self.valves = self.Valves()\n        self.name: str = \"Azure AI\"\n\n    def validate_environment(self) -> None:\n        \"\"\"\n        Validates that required environment variables are set.\n\n        Raises:\n            ValueError: If required environment variables are not set.\n        \"\"\"\n        # Access the decrypted API key\n        api_key = self.valves.AZURE_AI_API_KEY.get_decrypted()\n        if not api_key:\n            raise ValueError(\"AZURE_AI_API_KEY is not set!\")\n        if not self.valves.AZURE_AI_ENDPOINT:\n            raise ValueError(\"AZURE_AI_ENDPOINT is not set!\")\n\n    def get_headers(self, model_name: str = None) -> Dict[str, str]:\n        \"\"\"\n        Constructs the headers for the API request, including the model name if defined.\n\n        Args:\n            model_name: Optional model name to use instead of the default one\n\n        Returns:\n            Dictionary containing the required headers for the API request.\n        \"\"\"\n        # Access the decrypted API key\n        api_key = self.valves.AZURE_AI_API_KEY.get_decrypted()\n        if self.valves.USE_AUTHORIZATION_HEADER:\n            headers = {\n                \"Authorization\": f\"Bearer {api_key}\",\n                \"Content-Type\": \"application/json\",\n            }\n        else:\n            headers = {\"api-key\": api_key, \"Content-Type\": \"application/json\"}\n\n        # If we have a model name and it shouldn't be in the body, add it to headers\n        if not self.valves.AZURE_AI_MODEL_IN_BODY:\n            # If specific model name provided, use it\n            if model_name:\n                headers[\"x-ms-model-mesh-model-name\"] = model_name\n            # Otherwise, if AZURE_AI_MODEL has a single value, use that\n            elif (\n                self.valves.AZURE_AI_MODEL\n                and \";\" not in self.valves.AZURE_AI_MODEL\n                and \",\" not in self.valves.AZURE_AI_MODEL\n                and \" \" not in self.valves.AZURE_AI_MODEL\n            ):\n                headers[\"x-ms-model-mesh-model-name\"] = self.valves.AZURE_AI_MODEL\n        return headers\n\n    def validate_body(self, body: Dict[str, Any]) -> None:\n        \"\"\"\n        Validates the request body to ensure required fields are present.\n\n        Args:\n            body: The request body to validate\n\n        Raises:\n            ValueError: If required fields are missing or invalid.\n        \"\"\"\n        if \"messages\" not in body or not isinstance(body[\"messages\"], list):\n            raise ValueError(\"The 'messages' field is required and must be a list.\")\n\n    def parse_models(self, models_str: str) -> List[str]:\n        \"\"\"\n        Parses a string of models separated by commas, semicolons, or spaces.\n\n        Args:\n            models_str: String containing model names separated by commas, semicolons, or spaces\n\n        Returns:\n            List of individual model names\n        \"\"\"\n        if not models_str:\n            return []\n\n        # Replace semicolons and commas with spaces, then split by spaces and filter empty strings\n        models = []\n        for model in models_str.replace(\";\", \" \").replace(\",\", \" \").split():\n            if model.strip():\n                models.append(model.strip())\n\n        return models\n\n    def get_azure_models(self) -> List[Dict[str, str]]:\n        \"\"\"\n        Returns a list of predefined Azure AI models.\n\n        Returns:\n            List of dictionaries containing model id and name.\n        \"\"\"\n        return [\n            {\"id\": \"AI21-Jamba-1.5-Large\", \"name\": \"AI21 Jamba 1.5 Large\"},\n            {\"id\": \"AI21-Jamba-1.5-Mini\", \"name\": \"AI21 Jamba 1.5 Mini\"},\n            {\"id\": \"Codestral-2501\", \"name\": \"Codestral 25.01\"},\n            {\"id\": \"Cohere-command-r\", \"name\": \"Cohere Command R\"},\n            {\"id\": \"Cohere-command-r-08-2024\", \"name\": \"Cohere Command R 08-2024\"},\n            {\"id\": \"Cohere-command-r-plus\", \"name\": \"Cohere Command R+\"},\n            {\n                \"id\": \"Cohere-command-r-plus-08-2024\",\n                \"name\": \"Cohere Command R+ 08-2024\",\n            },\n            {\"id\": \"cohere-command-a\", \"name\": \"Cohere Command A\"},\n            {\"id\": \"DeepSeek-R1\", \"name\": \"DeepSeek-R1\"},\n            {\"id\": \"DeepSeek-V3\", \"name\": \"DeepSeek-V3\"},\n            {\"id\": \"jais-30b-chat\", \"name\": \"JAIS 30b Chat\"},\n            {\n                \"id\": \"Llama-3.2-11B-Vision-Instruct\",\n                \"name\": \"Llama-3.2-11B-Vision-Instruct\",\n            },\n            {\n                \"id\": \"Llama-3.2-90B-Vision-Instruct\",\n                \"name\": \"Llama-3.2-90B-Vision-Instruct\",\n            },\n            {\"id\": \"Llama-3.3-70B-Instruct\", \"name\": \"Llama-3.3-70B-Instruct\"},\n            {\"id\": \"Meta-Llama-3-70B-Instruct\", \"name\": \"Meta-Llama-3-70B-Instruct\"},\n            {\"id\": \"Meta-Llama-3-8B-Instruct\", \"name\": \"Meta-Llama-3-8B-Instruct\"},\n            {\n                \"id\": \"Meta-Llama-3.1-405B-Instruct\",\n                \"name\": \"Meta-Llama-3.1-405B-Instruct\",\n            },\n            {\n                \"id\": \"Meta-Llama-3.1-70B-Instruct\",\n                \"name\": \"Meta-Llama-3.1-70B-Instruct\",\n            },\n            {\"id\": \"Meta-Llama-3.1-8B-Instruct\", \"name\": \"Meta-Llama-3.1-8B-Instruct\"},\n            {\"id\": \"Ministral-3B\", \"name\": \"Ministral 3B\"},\n            {\"id\": \"Mistral-large\", \"name\": \"Mistral Large\"},\n            {\"id\": \"Mistral-large-2407\", \"name\": \"Mistral Large (2407)\"},\n            {\"id\": \"Mistral-Large-2411\", \"name\": \"Mistral Large 24.11\"},\n            {\"id\": \"Mistral-Nemo\", \"name\": \"Mistral Nemo\"},\n            {\"id\": \"Mistral-small\", \"name\": \"Mistral Small\"},\n            {\"id\": \"mistral-small-2503\", \"name\": \"Mistral Small 3.1\"},\n            {\"id\": \"gpt-4o\", \"name\": \"OpenAI GPT-4o\"},\n            {\"id\": \"gpt-4o-mini\", \"name\": \"OpenAI GPT-4o mini\"},\n            {\"id\": \"gpt-4.1\", \"name\": \"OpenAI GPT-4.1\"},\n            {\"id\": \"gpt-4.1-mini\", \"name\": \"OpenAI GPT-4.1 Mini\"},\n            {\"id\": \"gpt-4.1-nano\", \"name\": \"OpenAI GPT-4.1 Nano\"},\n            {\"id\": \"o1\", \"name\": \"OpenAI o1\"},\n            {\"id\": \"o1-mini\", \"name\": \"OpenAI o1-mini\"},\n            {\"id\": \"o1-preview\", \"name\": \"OpenAI o1-preview\"},\n            {\"id\": \"o3\", \"name\": \"OpenAI o3\"},\n            {\"id\": \"o3-mini\", \"name\": \"OpenAI o3-mini\"},\n            {\"id\": \"o4-mini\", \"name\": \"OpenAI o4-mini\"},\n            {\n                \"id\": \"Phi-3-medium-128k-instruct\",\n                \"name\": \"Phi-3-medium instruct (128k)\",\n            },\n            {\"id\": \"Phi-3-medium-4k-instruct\", \"name\": \"Phi-3-medium instruct (4k)\"},\n            {\"id\": \"Phi-3-mini-128k-instruct\", \"name\": \"Phi-3-mini instruct (128k)\"},\n            {\"id\": \"Phi-3-mini-4k-instruct\", \"name\": \"Phi-3-mini instruct (4k)\"},\n            {\"id\": \"Phi-3-small-128k-instruct\", \"name\": \"Phi-3-small instruct (128k)\"},\n            {\"id\": \"Phi-3-small-8k-instruct\", \"name\": \"Phi-3-small instruct (8k)\"},\n            {\"id\": \"Phi-3.5-mini-instruct\", \"name\": \"Phi-3.5-mini instruct (128k)\"},\n            {\"id\": \"Phi-3.5-MoE-instruct\", \"name\": \"Phi-3.5-MoE instruct (128k)\"},\n            {\"id\": \"Phi-3.5-vision-instruct\", \"name\": \"Phi-3.5-vision instruct (128k)\"},\n            {\"id\": \"Phi-4\", \"name\": \"Phi-4\"},\n            {\"id\": \"Phi-4-mini-instruct\", \"name\": \"Phi-4 mini instruct\"},\n            {\"id\": \"Phi-4-multimodal-instruct\", \"name\": \"Phi-4 multimodal instruct\"},\n        ]\n\n    def pipes(self) -> List[Dict[str, str]]:\n        \"\"\"\n        Returns a list of available pipes based on configuration.\n\n        Returns:\n            List of dictionaries containing pipe id and name.\n        \"\"\"\n        self.validate_environment()\n\n        # If custom models are provided, parse them and return as pipes\n        if self.valves.AZURE_AI_MODEL:\n            self.name = \"Azure AI: \"\n            models = self.parse_models(self.valves.AZURE_AI_MODEL)\n            if models:\n                return [{\"id\": model, \"name\": model} for model in models]\n            else:\n                # Fallback for backward compatibility\n                return [\n                    {\n                        \"id\": self.valves.AZURE_AI_MODEL,\n                        \"name\": self.valves.AZURE_AI_MODEL,\n                    }\n                ]\n\n        # If custom model is not provided but predefined models are enabled, return those.\n        if self.valves.USE_PREDEFINED_AZURE_AI_MODELS:\n            self.name = \"Azure AI: \"\n            return self.get_azure_models()\n\n        # Otherwise, use a default name.\n        return [{\"id\": \"Azure AI\", \"name\": \"Azure AI\"}]\n\n    async def pipe(\n        self, body: Dict[str, Any]\n    ) -> Union[str, Generator, Iterator, Dict[str, Any], StreamingResponse]:\n        \"\"\"\n        Main method for sending requests to the Azure AI endpoint.\n        The model name is passed as a header if defined.\n\n        Args:\n            body: The request body containing messages and other parameters\n\n        Returns:\n            Response from Azure AI API, which could be a string, dictionary or streaming response\n        \"\"\"\n        log = logging.getLogger(\"azure_ai.pipe\")\n        log.setLevel(SRC_LOG_LEVELS[\"OPENAI\"])\n\n        # Validate the request body\n        self.validate_body(body)\n        selected_model = None\n\n        if \"model\" in body and body[\"model\"]:\n            selected_model = body[\"model\"]\n            # Safer model extraction with split\n            selected_model = (\n                selected_model.split(\".\", 1)[1]\n                if \".\" in selected_model\n                else selected_model\n            )\n\n        # Construct headers with selected model\n        headers = self.get_headers(selected_model)\n\n        # Filter allowed parameters\n        allowed_params = {\n            \"model\",\n            \"messages\",\n            \"frequency_penalty\",\n            \"max_tokens\",\n            \"presence_penalty\",\n            \"response_format\",\n            \"seed\",\n            \"stop\",\n            \"stream\",\n            \"temperature\",\n            \"tool_choice\",\n            \"tools\",\n            \"top_p\",\n        }\n        filtered_body = {k: v for k, v in body.items() if k in allowed_params}\n\n        if self.valves.AZURE_AI_MODEL and self.valves.AZURE_AI_MODEL_IN_BODY:\n            # If a model was explicitly selected in the request, use that\n            if selected_model:\n                filtered_body[\"model\"] = selected_model\n            else:\n                # Otherwise, if AZURE_AI_MODEL contains multiple models, only use the first one to avoid errors\n                models = self.parse_models(self.valves.AZURE_AI_MODEL)\n                if models and len(models) > 0:\n                    filtered_body[\"model\"] = models[0]\n                else:\n                    # Fallback to the original value\n                    filtered_body[\"model\"] = self.valves.AZURE_AI_MODEL\n        elif \"model\" in filtered_body and filtered_body[\"model\"]:\n            # Safer model extraction with split\n            filtered_body[\"model\"] = (\n                filtered_body[\"model\"].split(\".\", 1)[1]\n                if \".\" in filtered_body[\"model\"]\n                else filtered_body[\"model\"]\n            )\n\n        # Convert the modified body back to JSON\n        payload = json.dumps(filtered_body)\n\n        request = None\n        session = None\n        streaming = False\n        response = None\n\n        try:\n            session = aiohttp.ClientSession(\n                trust_env=True,\n                timeout=aiohttp.ClientTimeout(total=AIOHTTP_CLIENT_TIMEOUT),\n            )\n\n            request = await session.request(\n                method=\"POST\",\n                url=self.valves.AZURE_AI_ENDPOINT,\n                data=payload,\n                headers=headers,\n            )\n\n            # Check if response is SSE\n            if \"text/event-stream\" in request.headers.get(\"Content-Type\", \"\"):\n                streaming = True\n                return StreamingResponse(\n                    request.content,\n                    status_code=request.status,\n                    headers=dict(request.headers),\n                    background=BackgroundTask(\n                        cleanup_response, response=request, session=session\n                    ),\n                )\n            else:\n                try:\n                    response = await request.json()\n                except Exception as e:\n                    log.error(f\"Error parsing JSON response: {e}\")\n                    response = await request.text()\n\n                request.raise_for_status()\n                return response\n\n        except Exception as e:\n            log.exception(f\"Error in Azure AI request: {e}\")\n\n            detail = f\"Exception: {str(e)}\"\n            if isinstance(response, dict):\n                if \"error\" in response:\n                    detail = f\"{response['error']['message'] if 'message' in response['error'] else response['error']}\"\n            elif isinstance(response, str):\n                detail = response\n\n            return f\"Error: {detail}\"\n        finally:\n            if not streaming and session:\n                if request:\n                    request.close()\n                await session.close()\n",
        "meta": {
            "description": "azure_ai",
            "manifest": {
                "title": "Azure AI Foundry Pipeline",
                "author": "owndev",
                "author_url": "https://github.com/owndev",
                "project_url": "https://github.com/owndev/Open-WebUI-Functions",
                "funding_url": "https://github.com/owndev/Open-WebUI-Functions",
                "version": "2.2.0",
                "license": "Apache License 2.0",
                "description": "A pipeline for interacting with Azure AI services, enabling seamless communication with various AI models via configurable headers and robust error handling. This includes support for Azure OpenAI models as well as other Azure AI models by dynamically managing headers and request configurations.",
                "features": ""
            }
        },
        "is_active": false,
        "is_global": false,
        "updated_at": 1745218282,
        "created_at": 1745218282
    }
]